{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "The goal of this notebook is to detect and replace the values of any loss of data\n",
    "\n",
    "## Sensor dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two types of outliers in out dataset, missing data aka NaN or data putted as 0.\n",
    "For both temperatures, it's pretty straightforwards because the temps never goes below 20 degrees (roughly), so detecting missing values is easy.\n",
    "\n",
    "But for irradiation, we have to detect where values at 0 shouldn't be there.\n",
    "\n",
    "UPDDATE 1: Noticed that I was wrong, no real value at 0 but lots of missing values, not NaN but missing, so need to remake the dataset to add those dates and times at those points.\n",
    "\n",
    "To do that, we'll \"split\" the dataset use Kmeans clustering, and detect all values at 0 in the middle of the day.\n",
    "\n",
    "Afterwards, we'll be using the \"cleaned\" set to apply feature engineering for each columns, and finally chose, train and use a model to replace each missing/wrong values\n",
    "\n",
    "Finally, we'll save the new dataset as parquet file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_prep import *\n",
    "from src.feature_engineering import *\n",
    "# Load both training and dataset used for end completion. \n",
    "dfs = load_sensor(\"dataset/Plant_1_Weather_Sensor_Data.csv\", save=False)\n",
    "# Get the X and y for all expectedd values\n",
    "irr_X, irr_y = sensor_for_irradiation(dfs[0])\n",
    "atemp_X, atemp_y = sensor_for_ambient_temp(dfs[0])\n",
    "mtemp_X, mtemp_y = sensor_for_module_temp(dfs[0])\n",
    "\n",
    "random_state_nb = 42\n",
    "test_size = 0.2\n",
    "\n",
    "irrX_train, irrX_dev, irry_train, irry_dev = train_test_split(irr_X, irr_y, test_size=test_size, random_state=random_state_nb)\n",
    "atempX_train, atempX_dev, atempy_train, atempy_dev = train_test_split(at_X, at_y, test_size=test_size, random_state=random_state_nb)\n",
    "mtempX_train, mtempX_dev, mtempy_train, mtempy_dev =  train_test_split(amX, amy, test_size=test_size, random_state=random_state_nb)\n",
    "\n",
    "\n",
    "Xs = {\n",
    "    'IRRADIATION': (irrX_train, irrX_dev),\n",
    "    'AMBIENT_TEMPERATURE': (atempX_train, atempX_dev),\n",
    "    'MODULE_TEMPERATURE': (mtempX_train, mtempX_dev),\n",
    "}\n",
    "\n",
    "ys = {\n",
    "    'IRRADIATION': (irry_train, irry_dev) ,\n",
    "    'AMBIENT_TEMPERATURE': (atempy_train, atempy_dev),\n",
    "    'MODULE_TEMPERATURE': (mtempy_train, mtempy_dev) ,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRRADIATION (Chosing best models)\n",
    "\n",
    "As suspecting non-linear relationships, I'll focus on those first models to take a grasp of our goals:\n",
    "- SVMs\n",
    "- DecisionTree\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "\n",
    "Additionnal models that can be noted for further tests:\n",
    "- Neural Nets\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "\n",
    "These metrics will be used to get a good overview of models performances on requested tasks:\n",
    "- R-Squared\n",
    "- MAE\n",
    "- MSE\n",
    "- RMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and optimize the best model\n",
    "from src.optimization import *\n",
    "\n",
    "irr_model = GradientBoostingRegressor()\n",
    "mtemp_model = GradientBoostingRegressor()\n",
    "atemp_model = GradientBoostingRegressor()\n",
    "\n",
    "model_d = {\n",
    "    'IRRADIATION': irr_model,\n",
    "    'AMBIENT_TEMPERATURE': mtemp_model,\n",
    "    'MODULE_TEMPERATURE': atemp_model,\n",
    "}\n",
    "\n",
    "\n",
    "for k,v in model_d.items():\n",
    "    model_d[k] = normgb_opt(Xs[k], ys[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying mdels\n",
    "\n",
    "changed_df = write_df(dfs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 40 candidates, totalling 80 fits\n",
      "{'C': 16, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "-----------SVM-----------\n",
      "r2:0.9570077536976176\n",
      "MAE:0.0392994736688664\n",
      "MSE:0.0036580574443085536\n",
      "RMSE:0.060481876990620534\n",
      "Fitting 2 folds for each of 16 candidates, totalling 32 fits\n",
      "{'criterion': 'squared_error', 'max_depth': 11, 'min_samples_split': 12, 'splitter': 'random'}\n",
      "-----------DT-----------\n",
      "r2:0.9566498064182145\n",
      "MAE:0.030151802033353967\n",
      "MSE:0.003701326460104872\n",
      "RMSE:0.06083852776082663\n",
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
      "{'criterion': 'squared_error', 'max_depth': 2, 'max_features': 'log2', 'min_samples_split': 2, 'n_estimators': 85}\n",
      "-----------RF-----------\n",
      "r2:0.9378672041932105\n",
      "MAE:0.045347806172349986\n",
      "MSE:0.004697701540228066\n",
      "RMSE:0.06853978071330595\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'min_samples_split': 2, 'n_estimators': 160, 'subsample': 0.6}\n",
      "-----------GB-----------\n",
      "r2:0.9609380623081305\n",
      "MAE:0.03149596724956426\n",
      "MSE:0.003303391858879091\n",
      "RMSE:0.05747514122539492\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.model import * \n",
    "mods = []\n",
    "for m in ['SVM', 'DT', 'RF', 'GB']:\n",
    "    model, metrics = model_testing(irr_X,irr_y, m)\n",
    "    mods.append(mods)\n",
    "    print(f'-----------{m}-----------')\n",
    "    print(f\"r2:{metrics['r2']}\\nMAE:{metrics['MAE']}\\nMSE:{metrics['MSE']}\\nRMSE:{metrics['RMSE']}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without surprise, Gradient boosting is one of the best models and goes toe to toe with randomforest (And slightly better if we take all metrics into consideration).\n",
    "And quite surprisingly, SVM and Decision Tree are also toe-to-toe but on the lower end.\n",
    "\n",
    "Now we'll try out adding some shallow tuning to see if there's any differences.\n",
    "\n",
    "**UPDATE 1:**\n",
    "Has it look like the computation time for my grids are way too high, reason are combination of many possibilities AND numbers are too high.\n",
    "\n",
    "**Example with SVR**: Computing C as 100 is twice or even thrices the computing time of 50.\n",
    "\n",
    "SO i'm putting my expectations way down to get a first line of sight of the parameters and improvement I could do.\n",
    "For some example of computations time tested (SO far for SVR (sight)) are twice 800 mins and one 1200, each reviewing the grid by nearly halfing it.\n",
    "\n",
    "**UPDATE 2:**\n",
    "The SVR computationnal issue was situated on the fact that a poly kernel doesn't fit well with gamma (need to expand on why exactly though)\n",
    "This was taking ages for nothings.\n",
    "Now should be resolved.|\n",
    "\n",
    "**UPDATE 3:**\n",
    "\n",
    "After simple tuning, we can bost SVM & DT to a way better fitting, for them to be closer to GB & RF.\n",
    "On the other hands, parameters used for fitting RF are not sufficient or too shallow, hence giving an overall worse prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1127\n",
      "[LightGBM] [Info] Number of data points in the train set: 2545, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 0.231840\n",
      "0\n",
      "0.9496143796207761\n",
      "0.9500916169818323\n",
      "0.03206345258992144\n",
      "0.03206345258992144\n",
      "0.004268907342379536\n",
      "0.0653368758235312\n",
      "1\n",
      "0.9588535545223675\n",
      "0.9591986696778222\n",
      "0.029951370402308206\n",
      "0.029951370402308206\n",
      "0.0034899367212076105\n",
      "0.05907568637948789\n",
      "2\n",
      "0.8671365902354395\n",
      "0.8587794302698107\n",
      "0.08877957665221778\n",
      "0.08877957665221778\n",
      "0.01207928389098027\n",
      "0.10990579552953643\n"
     ]
    }
   ],
   "source": [
    "# Trying out other libraries' \n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(irr_X, irr_y, test_size=0.2, random_state=42)\n",
    "extreme = xgb.XGBRegressor()\n",
    "light = lgb.LGBMRegressor()\n",
    "percep = MLPRegressor()\n",
    "\n",
    "extreme.fit(x_train, y_train)\n",
    "light.fit(x_train, y_train)\n",
    "percep.fit(x_train, y_train)\n",
    "\n",
    "preds  = []\n",
    "preds.append(extreme.predict(x_dev))\n",
    "preds.append(light.predict(x_dev))\n",
    "preds.append(percep.predict(x_dev))\n",
    "\n",
    "for index, p in enumerate(preds):\n",
    "    print(index)\n",
    "    print(r2_score(p, y_dev))\n",
    "    print(r2_score(y_dev, p))\n",
    "    print(mean_absolute_error(p, y_dev))\n",
    "    print(mean_absolute_error(y_dev, p))\n",
    "    print(mean_squared_error(y_dev,p, squared=True))\n",
    "    print(mean_squared_error(y_dev,p, squared=False))\n",
    "\n",
    "# Finishing by deeply tuning the expected candidate for best fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Plain lightgbm is working nearly as well as shallow tuned GB\n",
    "\n",
    "#### Final Tuning\n",
    "\n",
    "For finishing the full tuning process I'll be using random search technique as a part of the last parameter tuning while keeping in mind that the tuning part should be done within a night and/or within additional few hours.\n",
    "\n",
    "Will do it for Common Gradient Boosting and LightGBM as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Shadow\\Documents\\Artificial Intelligence\\SolarPW\\missliers.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shadow/Documents/Artificial%20Intelligence/SolarPW/missliers.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m irr_X, irr_y \u001b[39m=\u001b[39m sensor_for_irradiation(dfs[\u001b[39m0\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shadow/Documents/Artificial%20Intelligence/SolarPW/missliers.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m x_train, x_dev, y_train, y_dev \u001b[39m=\u001b[39m train_test_split(irr_X, irr_y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Shadow/Documents/Artificial%20Intelligence/SolarPW/missliers.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m params, model \u001b[39m=\u001b[39m normgb_opt(x_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\Documents\\Artificial Intelligence\\SolarPW\\src\\optimization.py:33\u001b[0m, in \u001b[0;36mnormgb_opt\u001b[1;34m(x_train, y_train)\u001b[0m\n\u001b[0;32m     20\u001b[0m     param_s \u001b[39m=\u001b[39m {\n\u001b[0;32m     21\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m: Integer(\u001b[39m50\u001b[39m, \u001b[39m200\u001b[39m),\n\u001b[0;32m     22\u001b[0m     }\n\u001b[0;32m     24\u001b[0m     search \u001b[39m=\u001b[39m BayesSearchCV(\n\u001b[0;32m     25\u001b[0m         GradientBoostingRegressor(), \n\u001b[0;32m     26\u001b[0m         param_s,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m     32\u001b[0m     )\n\u001b[1;32m---> 33\u001b[0m     search\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[0;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[39m# search.best_params_, search.best_estimator\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[39m# @use_named_args(s)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[39m# def objective_function(**params):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[39m# final_model = GradientBoostingRegressor(**best_hypp)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     \u001b[39m# final_model.fit(x_train, y_train)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\skopt\\searchcv.py:466\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[1;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_kwargs_ \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_kwargs)\n\u001b[1;32m--> 466\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, groups\u001b[39m=\u001b[39;49mgroups, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    468\u001b[0m \u001b[39m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_train_score:\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\skopt\\searchcv.py:512\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39mwhile\u001b[39;00m n_iter \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    509\u001b[0m     \u001b[39m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     n_points_adjusted \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_iter, n_points)\n\u001b[1;32m--> 512\u001b[0m     optim_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step(\n\u001b[0;32m    513\u001b[0m         search_space, optimizer,\n\u001b[0;32m    514\u001b[0m         evaluate_candidates, n_points\u001b[39m=\u001b[39;49mn_points_adjusted\n\u001b[0;32m    515\u001b[0m     )\n\u001b[0;32m    516\u001b[0m     n_iter \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m n_points\n\u001b[0;32m    518\u001b[0m     \u001b[39mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\skopt\\searchcv.py:400\u001b[0m, in \u001b[0;36mBayesSearchCV._step\u001b[1;34m(self, search_space, optimizer, evaluate_candidates, n_points)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate n_jobs parameters and evaluate them in parallel.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39m# get parameter values to evaluate\u001b[39;00m\n\u001b[1;32m--> 400\u001b[0m params \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39;49mask(n_points\u001b[39m=\u001b[39;49mn_points)\n\u001b[0;32m    402\u001b[0m \u001b[39m# convert parameters to python native types\u001b[39;00m\n\u001b[0;32m    403\u001b[0m params \u001b[39m=\u001b[39m [[np\u001b[39m.\u001b[39marray(v)\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m p] \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m params]\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:395\u001b[0m, in \u001b[0;36mOptimizer.ask\u001b[1;34m(self, n_points, strategy)\u001b[0m\n\u001b[0;32m    393\u001b[0m X \u001b[39m=\u001b[39m []\n\u001b[0;32m    394\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_points):\n\u001b[1;32m--> 395\u001b[0m     x \u001b[39m=\u001b[39m opt\u001b[39m.\u001b[39;49mask()\n\u001b[0;32m    396\u001b[0m     X\u001b[39m.\u001b[39mappend(x)\n\u001b[0;32m    398\u001b[0m     ti_available \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mps\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39macq_func \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(opt\u001b[39m.\u001b[39myi) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:367\u001b[0m, in \u001b[0;36mOptimizer.ask\u001b[1;34m(self, n_points, strategy)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Query point or multiple points at which objective should be evaluated.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \n\u001b[0;32m    338\u001b[0m \u001b[39mn_points : int or None, default: None\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    364\u001b[0m \n\u001b[0;32m    365\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[39mif\u001b[39;00m n_points \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ask()\n\u001b[0;32m    369\u001b[0m supported_strategies \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mcl_min\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcl_mean\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcl_max\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(n_points, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m n_points \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:434\u001b[0m, in \u001b[0;36mOptimizer._ask\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_initial_points \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_estimator_ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    431\u001b[0m     \u001b[39m# this will not make a copy of `self.rng` and hence keep advancing\u001b[39;00m\n\u001b[0;32m    432\u001b[0m     \u001b[39m# our random state.\u001b[39;00m\n\u001b[0;32m    433\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_samples \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 434\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspace\u001b[39m.\u001b[39;49mrvs(random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrng)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    435\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m         \u001b[39m# The samples are evaluated starting form initial_samples[0]\u001b[39;00m\n\u001b[0;32m    437\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_samples[\n\u001b[0;32m    438\u001b[0m             \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_samples) \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_initial_points]\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\skopt\\space\\space.py:900\u001b[0m, in \u001b[0;36mSpace.rvs\u001b[1;34m(self, n_samples, random_state)\u001b[0m\n\u001b[0;32m    897\u001b[0m columns \u001b[39m=\u001b[39m []\n\u001b[0;32m    899\u001b[0m \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdimensions:\n\u001b[1;32m--> 900\u001b[0m     columns\u001b[39m.\u001b[39mappend(dim\u001b[39m.\u001b[39;49mrvs(n_samples\u001b[39m=\u001b[39;49mn_samples, random_state\u001b[39m=\u001b[39;49mrng))\n\u001b[0;32m    902\u001b[0m \u001b[39m# Transpose\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[39mreturn\u001b[39;00m _transpose_list_array(columns)\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\skopt\\space\\space.py:158\u001b[0m, in \u001b[0;36mDimension.rvs\u001b[1;34m(self, n_samples, random_state)\u001b[0m\n\u001b[0;32m    156\u001b[0m rng \u001b[39m=\u001b[39m check_random_state(random_state)\n\u001b[0;32m    157\u001b[0m samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rvs\u001b[39m.\u001b[39mrvs(size\u001b[39m=\u001b[39mn_samples, random_state\u001b[39m=\u001b[39mrng)\n\u001b[1;32m--> 158\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minverse_transform(samples)\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\skopt\\space\\space.py:528\u001b[0m, in \u001b[0;36mInteger.inverse_transform\u001b[1;34m(self, Xt)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Inverse transform samples from the warped space back into the\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[39m   original space.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \u001b[39m# The concatenation of all transformed dimensions makes Xt to be\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[39m# of type float, hence the required cast back to int.\u001b[39;00m\n\u001b[1;32m--> 528\u001b[0m inv_transform \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(Integer, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49minverse_transform(Xt)\n\u001b[0;32m    529\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inv_transform, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    530\u001b[0m     inv_transform \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(inv_transform)\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\skopt\\space\\space.py:168\u001b[0m, in \u001b[0;36mDimension.inverse_transform\u001b[1;34m(self, Xt)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minverse_transform\u001b[39m(\u001b[39mself\u001b[39m, Xt):\n\u001b[0;32m    165\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Inverse transform samples from the warped space back into the\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[39m       original space.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39;49minverse_transform(Xt)\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\skopt\\space\\transformers.py:309\u001b[0m, in \u001b[0;36mPipeline.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minverse_transform\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    308\u001b[0m     \u001b[39mfor\u001b[39;00m transformer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformers[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m--> 309\u001b[0m         X \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49minverse_transform(X)\n\u001b[0;32m    310\u001b[0m     \u001b[39mreturn\u001b[39;00m X\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\skopt\\space\\transformers.py:275\u001b[0m, in \u001b[0;36mNormalize.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    273\u001b[0m X_orig \u001b[39m=\u001b[39m X \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhigh \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow\n\u001b[0;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_int:\n\u001b[1;32m--> 275\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mround(X_orig)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39;49mint)\n\u001b[0;32m    276\u001b[0m \u001b[39mreturn\u001b[39;00m X_orig\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\tgpu\\Lib\\site-packages\\numpy\\__init__.py:313\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    308\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    309\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn the future `np.\u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m` will be defined as the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorresponding NumPy scalar.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    312\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 313\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    315\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtesting\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    316\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtesting\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from src.optimization import *\n",
    "from src.data_prep import *\n",
    "from src.feature_engineering import *\n",
    "\n",
    "dfs = load_sensor(\"dataset/Plant_1_Weather_Sensor_Data.csv\", save=False)\n",
    "irr_X, irr_y = sensor_for_irradiation(dfs[0])\n",
    "\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(irr_X, irr_y, test_size=0.2, random_state=42)\n",
    "params, model = normgb_opt(x_train, y_train)\n",
    "\n",
    "# Getting dependencies issues for numpy and skopt, might need to change back to grid/randomsearch  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing & Displaying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing if getting the same values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
